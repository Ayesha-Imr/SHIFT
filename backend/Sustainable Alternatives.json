{"id":"cd360ace-d333-41a9-a85c-e661a6733fd3","data":{"nodes":[{"id":"GroqModel-3vsvr","type":"genericNode","position":{"x":-7.428471403758408,"y":145.88369710181172},"data":{"type":"GroqModel","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain_groq import ChatGroq\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.constants import STREAM_INFO_TEXT\nfrom langflow.base.models.groq_constants import MODEL_NAMES\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.io import BoolInput, DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput\n\n\nclass GroqModel(LCModelComponent):\n    display_name: str = \"Groq\"\n    description: str = \"Generate text using Groq.\"\n    icon = \"Groq\"\n    name = \"GroqModel\"\n\n    inputs = [\n        SecretStrInput(\n            name=\"groq_api_key\",\n            display_name=\"Groq API Key\",\n            info=\"API key for the Groq API.\",\n        ),\n        MessageTextInput(\n            name=\"groq_api_base\",\n            display_name=\"Groq API Base\",\n            info=\"Base URL path for API requests, leave blank if not using a proxy or service emulator.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Output Tokens\",\n            info=\"The maximum number of tokens to generate.\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            info=\"Run inference with this temperature. Must by in the closed interval [0.0, 1.0].\",\n            value=0.1,\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=MODEL_NAMES,\n        ),\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Input\",\n            info=\"The input to the model.\",\n        ),\n        BoolInput(\n            name=\"stream\",\n            display_name=\"Stream\",\n            info=STREAM_INFO_TEXT,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"System message to pass to the model.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        groq_api_key = self.groq_api_key\n        model_name = self.model_name\n        max_tokens = self.max_tokens\n        temperature = self.temperature\n        groq_api_base = self.groq_api_base\n        n = self.n\n        stream = self.stream\n\n        output = ChatGroq(  # type: ignore\n            model=model_name,\n            max_tokens=max_tokens or None,\n            temperature=temperature,\n            base_url=groq_api_base,\n            n=n or 1,\n            api_key=SecretStr(groq_api_key),\n            streaming=stream,\n        )\n\n        return output  # type: ignore\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"groq_api_base":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"groq_api_base","display_name":"Groq API Base","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Base URL path for API requests, leave blank if not using a proxy or service emulator.","title_case":false,"type":"str"},"groq_api_key":{"load_from_db":false,"required":false,"placeholder":"","show":true,"value":"","name":"groq_api_key","display_name":"Groq API Key","advanced":false,"input_types":[],"dynamic":false,"info":"API key for the Groq API.","title_case":false,"password":true,"type":"str"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The input to the model.","title_case":false,"type":"str"},"max_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"max_tokens","display_name":"Max Output Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate.","title_case":false,"type":"int"},"model_name":{"trace_as_metadata":true,"options":["llama3-8b-8192","llama3-70b-8192","mixtral-8x7b-32768","gemma-7b-it"],"required":false,"placeholder":"","show":true,"value":"llama3-70b-8192","name":"model_name","display_name":"Model","advanced":false,"dynamic":false,"info":"The name of the model to use.","title_case":false,"type":"str"},"n":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"n","display_name":"N","advanced":true,"dynamic":false,"info":"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.","title_case":false,"type":"int"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"stream","display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system_message","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":0.1,"name":"temperature","display_name":"Temperature","advanced":false,"dynamic":false,"info":"Run inference with this temperature. Must by in the closed interval [0.0, 1.0].","title_case":false,"type":"float"}},"description":"Generate text using Groq.","icon":"Groq","base_classes":["LanguageModel","Message"],"display_name":"Groq","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["groq_api_key","groq_api_base","max_tokens","temperature","n","model_name","input_value","stream","system_message"],"beta":false,"edited":false},"id":"GroqModel-3vsvr"},"selected":false,"width":384,"height":637,"positionAbsolute":{"x":-7.428471403758408,"y":145.88369710181172},"dragging":false},{"id":"ToolCallingAgent-AYS28","type":"genericNode","position":{"x":915.3559962997248,"y":392.61867751618024},"data":{"type":"ToolCallingAgent","node":{"template":{"_type":"Component","llm":{"trace_as_metadata":true,"list":false,"required":true,"placeholder":"","show":true,"value":"","name":"llm","display_name":"Language Model","advanced":false,"input_types":["LanguageModel"],"dynamic":false,"info":"","title_case":false,"type":"other","load_from_db":false},"tools":{"trace_as_metadata":true,"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"tools","display_name":"Tools","advanced":false,"input_types":["Tool"],"dynamic":false,"info":"","title_case":false,"type":"other","load_from_db":false},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain.agents import create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, PromptTemplate, HumanMessagePromptTemplate\nfrom langflow.base.agents.agent import LCToolsAgentComponent\nfrom langflow.inputs import MultilineInput\n\n\nclass ToolCallingAgentComponent(LCToolsAgentComponent):\n    display_name: str = \"Tool Calling Agent\"\n    description: str = \"Agent that uses tools\"\n    icon = \"LangChain\"\n    beta = True\n    name = \"ToolCallingAgent\"\n\n    inputs = LCToolsAgentComponent._base_inputs + [\n        MultilineInput(\n            name=\"system_prompt\",\n            display_name=\"System Prompt\",\n            info=\"System prompt for the agent.\",\n            value=\"You are a helpful assistant\",\n        ),\n        MultilineInput(\n            name=\"user_prompt\", display_name=\"Prompt\", info=\"This prompt must contain 'input' key.\", value=\"\"\n        ),\n    ]\n\n    def creat_agent_runnable(self):\n        messages = [\n            (\"system\", self.system_prompt),\n            HumanMessagePromptTemplate(prompt=PromptTemplate(template=self.user_prompt)),\n            (\"placeholder\", \"{agent_scratchpad}\"),\n        ]\n        prompt = ChatPromptTemplate.from_messages(messages)\n        return create_tool_calling_agent(self.llm, self.tools, prompt)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"handle_parsing_errors":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":true,"name":"handle_parsing_errors","display_name":"Handle Parse Errors","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"bool"},"max_iterations":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":15,"name":"max_iterations","display_name":"Max Iterations","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"int"},"system_prompt":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"You are a helpful assistant whose task is to search the internet for alternatives to the product whose description is given to you. The alternatives should be more environmental-friendly than this product and should have less negative impact on the climate. Do not write brand names in your search query not make it very narrow - be as general as possible in order to return a wide range of results. If you find links to alternatives, do mention those. Otherwise, clearly mention the product names and, if possible, brand names and purchase options so the user has the maximum amount of information and resources needed to purchase the item(s).","name":"system_prompt","display_name":"System Prompt","advanced":false,"input_types":["Message"],"dynamic":false,"info":"System prompt for the agent.","title_case":false,"type":"str"},"user_prompt":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"user_prompt","display_name":"Prompt","advanced":false,"input_types":["Message"],"dynamic":false,"info":"This prompt must contain 'input' key.","title_case":false,"type":"str"},"verbose":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":true,"name":"verbose","display_name":"Verbose","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"bool"}},"description":"Agent that uses tools","icon":"LangChain","base_classes":["AgentExecutor"],"display_name":"Tool Calling Agent","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["AgentExecutor"],"selected":"AgentExecutor","name":"agent","display_name":"Agent","method":"build_agent","value":"__UNDEFINED__","cache":true}],"field_order":["handle_parsing_errors","verbose","max_iterations","tools","llm","system_prompt","user_prompt"],"beta":true,"edited":true},"id":"ToolCallingAgent-AYS28","description":"Agent that uses tools","display_name":"Tool Calling Agent"},"selected":false,"width":384,"height":498,"positionAbsolute":{"x":915.3559962997248,"y":392.61867751618024},"dragging":false},{"id":"SearchAPITool-XFHCW","type":"genericNode","position":{"x":-10.535693491073971,"y":-453.8092453149164},"data":{"type":"SearchAPITool","node":{"template":{"_type":"CustomComponent","api_key":{"type":"str","required":true,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":true,"name":"api_key","display_name":"API Key","advanced":false,"dynamic":false,"info":"The API key to use SearchApi.","load_from_db":false,"title_case":false,"input_types":["Text"],"value":""},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain_community.tools.searchapi import SearchAPIRun\nfrom langchain_community.utilities.searchapi import SearchApiAPIWrapper\n\nfrom langflow.custom import CustomComponent\nfrom langflow.field_typing import Tool\n\n\nclass SearchApiToolComponent(CustomComponent):\n    display_name: str = \"SearchApi Tool\"\n    description: str = \"Real-time search engine results API.\"\n    name = \"SearchAPITool\"\n    documentation: str = \"https://www.searchapi.io/docs/google\"\n    field_config = {\n        \"engine\": {\n            \"display_name\": \"Engine\",\n            \"field_type\": \"str\",\n            \"info\": \"The search engine to use.\",\n        },\n        \"api_key\": {\n            \"display_name\": \"API Key\",\n            \"field_type\": \"str\",\n            \"required\": True,\n            \"password\": True,\n            \"info\": \"The API key to use SearchApi.\",\n        },\n    }\n\n    def build(\n        self,\n        engine: str,\n        api_key: str,\n    ) -> Tool:\n        search_api_wrapper = SearchApiAPIWrapper(engine=engine, searchapi_api_key=api_key)\n\n        tool = SearchAPIRun(api_wrapper=search_api_wrapper)\n\n        self.status = tool\n        return tool  # type: ignore\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"engine":{"type":"str","required":true,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"engine","display_name":"Engine","advanced":false,"dynamic":false,"info":"The search engine to use.","load_from_db":false,"title_case":false,"input_types":["Text"],"value":"Google"}},"description":"Real-time search engine results API.","base_classes":["BaseTool","Generic","object","Runnable","RunnableSerializable","Serializable","Tool"],"display_name":"SearchApi Tool","documentation":"https://www.searchapi.io/docs/google","custom_fields":{"engine":null,"api_key":null},"output_types":["Tool"],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Tool"],"selected":"Tool","name":"tool","hidden":null,"display_name":"Tool","method":null,"value":"__UNDEFINED__","cache":true}],"field_order":["engine","api_key"],"beta":false,"edited":false},"id":"SearchAPITool-XFHCW"},"selected":false,"width":384,"height":402,"positionAbsolute":{"x":-10.535693491073971,"y":-453.8092453149164},"dragging":false},{"id":"RunnableExecutor-TiNym","type":"genericNode","position":{"x":1772.7485054262104,"y":110.06115186188725},"data":{"type":"RunnableExecutor","node":{"template":{"_type":"Component","runnable":{"trace_as_metadata":true,"list":false,"required":true,"placeholder":"","show":true,"value":"","name":"runnable","display_name":"Agent Executor","advanced":false,"input_types":["Chain","AgentExecutor","Agent","Runnable"],"dynamic":false,"info":"","title_case":false,"type":"other"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.inputs import HandleInput, MessageTextInput\nfrom langflow.schema.message import Message\nfrom langflow.template import Output\n\n\nclass RunnableExecComponent(Component):\n    description = \"Execute a runnable. It will try to guess the input and output keys.\"\n    display_name = \"Runnable Executor\"\n    name = \"RunnableExecutor\"\n    beta: bool = True\n\n    inputs = [\n        MessageTextInput(name=\"input_value\", display_name=\"Input\", required=True),\n        HandleInput(\n            name=\"runnable\",\n            display_name=\"Agent Executor\",\n            input_types=[\"Chain\", \"AgentExecutor\", \"Agent\", \"Runnable\"],\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"input_key\",\n            display_name=\"Input Key\",\n            value=\"input\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"output_key\",\n            display_name=\"Output Key\",\n            value=\"output\",\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(\n            display_name=\"Text\",\n            name=\"text\",\n            method=\"build_executor\",\n        ),\n    ]\n\n    def get_output(self, result, input_key, output_key):\n        \"\"\"\n        Retrieves the output value from the given result dictionary based on the specified input and output keys.\n\n        Args:\n            result (dict): The result dictionary containing the output value.\n            input_key (str): The key used to retrieve the input value from the result dictionary.\n            output_key (str): The key used to retrieve the output value from the result dictionary.\n\n        Returns:\n            tuple: A tuple containing the output value and the status message.\n\n        \"\"\"\n        possible_output_keys = [\"answer\", \"response\", \"output\", \"result\", \"text\"]\n        status = \"\"\n        result_value = None\n\n        if output_key in result:\n            result_value = result.get(output_key)\n        elif len(result) == 2 and input_key in result:\n            # get the other key from the result dict\n            other_key = [k for k in result if k != input_key][0]\n            if other_key == output_key:\n                result_value = result.get(output_key)\n            else:\n                status += f\"Warning: The output key is not '{output_key}'. The output key is '{other_key}'.\"\n                result_value = result.get(other_key)\n        elif len(result) == 1:\n            result_value = list(result.values())[0]\n        elif any(k in result for k in possible_output_keys):\n            for key in possible_output_keys:\n                if key in result:\n                    result_value = result.get(key)\n                    status += f\"Output key: '{key}'.\"\n                    break\n            if result_value is None:\n                result_value = result\n                status += f\"Warning: The output key is not '{output_key}'.\"\n        else:\n            result_value = result\n            status += f\"Warning: The output key is not '{output_key}'.\"\n\n        return result_value, status\n\n    def get_input_dict(self, runnable, input_key, input_value):\n        \"\"\"\n        Returns a dictionary containing the input key-value pair for the given runnable.\n\n        Args:\n            runnable: The runnable object.\n            input_key: The key for the input value.\n            input_value: The value for the input key.\n\n        Returns:\n            input_dict: A dictionary containing the input key-value pair.\n            status: A status message indicating if the input key is not in the runnable's input keys.\n        \"\"\"\n        input_dict = {}\n        status = \"\"\n        if hasattr(runnable, \"input_keys\"):\n            # Check if input_key is in the runnable's input_keys\n            if input_key in runnable.input_keys:\n                input_dict[input_key] = input_value\n            else:\n                input_dict = {k: input_value for k in runnable.input_keys}\n                status = f\"Warning: The input key is not '{input_key}'. The input key is '{runnable.input_keys}'.\"\n        return input_dict, status\n\n    def build_executor(self) -> Message:\n        input_dict, status = self.get_input_dict(self.runnable, self.input_key, self.input_value)\n        result = self.runnable.invoke(input_dict)\n        result_value, _status = self.get_output(result, self.input_key, self.output_key)\n        status += _status\n        status += f\"\\n\\nOutput: {result_value}\\n\\nRaw Output: {result}\"\n        self.status = status\n        return result_value\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_key":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"input","name":"input_key","display_name":"Input Key","advanced":true,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":true,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"},"output_key":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"output","name":"output_key","display_name":"Output Key","advanced":true,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"}},"description":"Execute a runnable. It will try to guess the input and output keys.","base_classes":["Message"],"display_name":"Runnable Executor","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"build_executor","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","runnable","input_key","output_key"],"beta":true,"edited":false},"id":"RunnableExecutor-TiNym"},"selected":false,"width":384,"height":384,"positionAbsolute":{"x":1772.7485054262104,"y":110.06115186188725},"dragging":false},{"id":"ChatInput-55Ydu","type":"genericNode","position":{"x":888.6051344597704,"y":-368.8742661519867},"data":{"type":"ChatInput","node":{"template":{"_type":"Component","files":{"trace_as_metadata":true,"file_path":"","fileTypes":["txt","md","mdx","csv","json","yaml","yml","xml","html","htm","pdf","docx","py","sh","sql","js","ts","tsx","jpg","jpeg","png","bmp","image"],"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"files","display_name":"Files","advanced":true,"dynamic":false,"info":"Files to be sent with the message.","title_case":false,"type":"file"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"User\",\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=\"User\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as input.","title_case":false,"type":"str"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"required":false,"placeholder":"","show":true,"value":"User","name":"sender","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"User","name":"sender_name","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"session_id","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Session ID for the message.","title_case":false,"type":"str"},"store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":true,"name":"store_message","display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool"}},"description":"Get chat inputs from the Playground.","icon":"ChatInput","base_classes":["Message"],"display_name":"Chat Input","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","store_message","sender","sender_name","session_id","files"],"beta":false,"edited":false},"id":"ChatInput-55Ydu"},"selected":false,"width":384,"height":308,"positionAbsolute":{"x":888.6051344597704,"y":-368.8742661519867},"dragging":false},{"id":"ChatOutput-JQIVA","type":"genericNode","position":{"x":2492.198959423342,"y":73.61964599943617},"data":{"type":"ChatOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"Machine\",\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\", display_name=\"Sender Name\", info=\"Name of the sender.\", value=\"AI\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"data_template":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"{text}","name":"data_template","display_name":"Data Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","title_case":false,"type":"str"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as output.","title_case":false,"type":"str"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"required":false,"placeholder":"","show":true,"value":"Machine","name":"sender","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"AI","name":"sender_name","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"session_id","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Session ID for the message.","title_case":false,"type":"str"},"store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":true,"name":"store_message","display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool"}},"description":"Display a chat message in the Playground.","icon":"ChatOutput","base_classes":["Message"],"display_name":"Chat Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","store_message","sender","sender_name","session_id","data_template"],"beta":false,"edited":false},"id":"ChatOutput-JQIVA"},"selected":false,"width":384,"height":308,"dragging":false}],"edges":[{"source":"GroqModel-3vsvr","sourceHandle":"{œdataTypeœ:œGroqModelœ,œidœ:œGroqModel-3vsvrœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}","target":"ToolCallingAgent-AYS28","targetHandle":"{œfieldNameœ:œllmœ,œidœ:œToolCallingAgent-AYS28œ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"llm","id":"ToolCallingAgent-AYS28","inputTypes":["LanguageModel"],"type":"other"},"sourceHandle":{"dataType":"GroqModel","id":"GroqModel-3vsvr","name":"model_output","output_types":["LanguageModel"]}},"id":"reactflow__edge-GroqModel-3vsvr{œdataTypeœ:œGroqModelœ,œidœ:œGroqModel-3vsvrœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-ToolCallingAgent-AYS28{œfieldNameœ:œllmœ,œidœ:œToolCallingAgent-AYS28œ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}"},{"source":"SearchAPITool-XFHCW","sourceHandle":"{œdataTypeœ:œSearchAPIToolœ,œidœ:œSearchAPITool-XFHCWœ,œnameœ:œtoolœ,œoutput_typesœ:[œToolœ]}","target":"ToolCallingAgent-AYS28","targetHandle":"{œfieldNameœ:œtoolsœ,œidœ:œToolCallingAgent-AYS28œ,œinputTypesœ:[œToolœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"tools","id":"ToolCallingAgent-AYS28","inputTypes":["Tool"],"type":"other"},"sourceHandle":{"dataType":"SearchAPITool","id":"SearchAPITool-XFHCW","name":"tool","output_types":["Tool"]}},"id":"reactflow__edge-SearchAPITool-XFHCW{œdataTypeœ:œSearchAPIToolœ,œidœ:œSearchAPITool-XFHCWœ,œnameœ:œtoolœ,œoutput_typesœ:[œToolœ]}-ToolCallingAgent-AYS28{œfieldNameœ:œtoolsœ,œidœ:œToolCallingAgent-AYS28œ,œinputTypesœ:[œToolœ],œtypeœ:œotherœ}"},{"source":"ToolCallingAgent-AYS28","sourceHandle":"{œdataTypeœ:œToolCallingAgentœ,œidœ:œToolCallingAgent-AYS28œ,œnameœ:œagentœ,œoutput_typesœ:[œAgentExecutorœ]}","target":"RunnableExecutor-TiNym","targetHandle":"{œfieldNameœ:œrunnableœ,œidœ:œRunnableExecutor-TiNymœ,œinputTypesœ:[œChainœ,œAgentExecutorœ,œAgentœ,œRunnableœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"runnable","id":"RunnableExecutor-TiNym","inputTypes":["Chain","AgentExecutor","Agent","Runnable"],"type":"other"},"sourceHandle":{"dataType":"ToolCallingAgent","id":"ToolCallingAgent-AYS28","name":"agent","output_types":["AgentExecutor"]}},"id":"reactflow__edge-ToolCallingAgent-AYS28{œdataTypeœ:œToolCallingAgentœ,œidœ:œToolCallingAgent-AYS28œ,œnameœ:œagentœ,œoutput_typesœ:[œAgentExecutorœ]}-RunnableExecutor-TiNym{œfieldNameœ:œrunnableœ,œidœ:œRunnableExecutor-TiNymœ,œinputTypesœ:[œChainœ,œAgentExecutorœ,œAgentœ,œRunnableœ],œtypeœ:œotherœ}"},{"source":"ChatInput-55Ydu","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-55Yduœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"RunnableExecutor-TiNym","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œRunnableExecutor-TiNymœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"RunnableExecutor-TiNym","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-55Ydu","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-55Ydu{œdataTypeœ:œChatInputœ,œidœ:œChatInput-55Yduœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-RunnableExecutor-TiNym{œfieldNameœ:œinput_valueœ,œidœ:œRunnableExecutor-TiNymœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"},{"source":"RunnableExecutor-TiNym","sourceHandle":"{œdataTypeœ:œRunnableExecutorœ,œidœ:œRunnableExecutor-TiNymœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"ChatOutput-JQIVA","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-JQIVAœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-JQIVA","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"RunnableExecutor","id":"RunnableExecutor-TiNym","name":"text","output_types":["Message"]}},"id":"reactflow__edge-RunnableExecutor-TiNym{œdataTypeœ:œRunnableExecutorœ,œidœ:œRunnableExecutor-TiNymœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-JQIVA{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-JQIVAœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"},{"source":"ChatInput-55Ydu","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-55Yduœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"ToolCallingAgent-AYS28","targetHandle":"{œfieldNameœ:œuser_promptœ,œidœ:œToolCallingAgent-AYS28œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"user_prompt","id":"ToolCallingAgent-AYS28","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-55Ydu","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-55Ydu{œdataTypeœ:œChatInputœ,œidœ:œChatInput-55Yduœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-ToolCallingAgent-AYS28{œfieldNameœ:œuser_promptœ,œidœ:œToolCallingAgent-AYS28œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"}],"viewport":{"x":-190.24030454106617,"y":187.03521112847938,"zoom":0.4593567386800525}},"description":"Get suggestions for more eco-friendly alternatives to the product you want to purchase.","name":"Sustainable Alternatives","last_tested_version":"1.0.9","endpoint_name":null,"is_component":false}